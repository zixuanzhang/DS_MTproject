---
title: "revise_laoma"
author: "Eleanor Zhang"
date: "5/15/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width = 7,fig.asp = .7,out.width = "90%",
                      message = FALSE, warning = FALSE)
library(tidyverse)
library(ggridges)
library(caret)
library(boot)
library(pls)
library(glmnet)
library(splines)
library(mgcv)
library(MASS)
library(pROC)
library(GGally)
library(RColorBrewer)
library(randomForest) 
library(ranger)
library(ggpubr)
theme_set(theme_classic())
library(gbm)
# parallel processing with caret
library(doParallel)
cluster <- makePSOCKcluster(10)
registerDoParallel(cluster)
```

## Data

numeric binary outcome
```{r}
apple <- read_csv("./data/AppleStore.csv") %>%
  janitor::clean_names() %>%
  dplyr::select(-c(x1, id, track_name, currency, ver)) %>%
  mutate(size_bytes = round(size_bytes * 1e-6),
         cont_rating = factor(cont_rating, levels = c("4+", "9+","12+","17+")),
         user_rating = ifelse(user_rating >= 4, 1, 0)) %>%
  rename(size_megabytes = size_bytes) %>%
  filter(rating_count_tot != 0,
         user_rating_ver != 0) %>% #Remove apps with no user rating
  mutate(prime_genre = as.integer(ifelse(prime_genre == "Games", 1, 0)),
         user_rating = as.numeric(user_rating)) %>% 
  dplyr::select(-rating_count_tot, -rating_count_ver, -vpp_lic) #vpp_lic has nearzero variance

set.seed(1234)
#Split data to traning and testing
trRows = createDataPartition(apple$user_rating,
                             p = .75,
                             list = FALSE)
train_data = apple[trRows,]
test_data = apple[-trRows,]
#in matrix form
x_train = model.matrix(user_rating~., train_data)[,-1] 
y_train = train_data$user_rating
x_test = model.matrix(user_rating~., test_data)[,-1] 
y_test = test_data$user_rating
```

change outcome to factor and new names
```{r}
apple2 <- read_csv("./data/AppleStore.csv") %>%
  janitor::clean_names() %>%
  dplyr::select(-c(x1, id, track_name, currency, ver)) %>%
  mutate(size_bytes = round(size_bytes * 1e-6),
         cont_rating = factor(cont_rating, levels = c("4+", "9+","12+","17+")),
         user_rating = factor(ifelse(user_rating >= 4, "high", "med.low"), levels = c("med.low","high"))) %>%
  rename(size_megabytes = size_bytes) %>%
  filter(rating_count_tot != 0,
         user_rating_ver != 0) %>% #Remove apps with no user rating
  mutate(prime_genre = as.integer(ifelse(prime_genre == "Games", 1, 0))) %>% 
  dplyr::select(-rating_count_tot, -rating_count_ver, -vpp_lic)

set.seed(1234)
#Split data to traning and testing
trRows2 = createDataPartition(apple2$user_rating,
                             p = .75,
                             list = FALSE)
```

CV methods
```{r}
ctrl1 <- trainControl(method = "cv", number = 10)
ctrl2 <- trainControl(method = "cv",
                     summaryFunction = twoClassSummary, # use ROC curve as summary
                     classProbs = TRUE)
```

## EDA
```{r}
apple2 %>% 
  ggplot(aes(x = user_rating)) +
  geom_bar()

transparentTheme(trans = .4)
featurePlot(x = apple[, c(2:4, 7:9)], 
            y = apple$user_rating,
            scales = list(x = list(relation = "free"), 
                        y = list(relation = "free")),
            plot = "density", pch = "|", 
            auto.key = list(columns = 2))


```


## Decision Boundaries

### logistic regression

prepare data
```{r}
str(apple)
apple <- apple %>% dplyr::select(user_rating, everything())
apple2 <- apple2 %>% dplyr::select(user_rating, everything())

set.seed(1234)
model.glm <- train(user_rating ~., data = apple2,
                   subset = trRows2,
                   method = "glm",
                   metric = "ROC",
                   trControl = ctrl2)
summary(model.glm)
```

### glmnet

```{r}
glmnGrid <- expand.grid(.alpha = seq(0, 1, length = 6),
                        .lambda = exp(seq(-8, -5, length = 20)))
set.seed(1234)
model.glmn <- train(user_rating ~., data = apple2,
                    subset = trRows2,
                    method = "glmnet",
                    tuneGrid = glmnGrid,
                    metric = "ROC",
                    trControl = ctrl2)

model.glmn$bestTune

plot(model.glmn, xTrans = function(x) log(x), highlight = TRUE) 
```

### gam 

### LDA and QDA

LDA
```{r}
set.seed(1234)
model.lda <- train(user_rating ~., data = apple2,
                  subset = trRows2,
                   method = "lda",
                   metric = "ROC",
                   trControl = ctrl2)
```

QDA
```{r}
set.seed(1234)
model.qda <- train(user_rating ~., data = apple2,
                   subset = trRows2,
                   method = "qda",
                   metric = "ROC",
                   trControl = ctrl2)
```

### Naive Bayes
```{r}
set.seed(1234)
nbGrid <- expand.grid(usekernel = c(FALSE,TRUE),
                      fL = 1, 
                      adjust = seq(0,5,by = 1))

model.nb <- train(user_rating ~., data = apple2,
                  subset = trRows2,
                  method = "nb",
                  tuneGrid = nbGrid,
                  metric = "ROC",
                  trControl = ctrl2)

plot(model.nb)
```

### KNN

```{r}
set.seed(1234)
model.knn <- train(user_rating ~., data = apple2,
                   subset = trRows2,
                   method = "knn",
                   preProcess = c("center","scale"),
                   tuneGrid = data.frame(k = seq(1,200,by=5)),
                   trControl = ctrl2)

ggplot(model.knn)
```


## Tree based

```{r}
library(rpart) # for recursive partition (CART: Classfification and Regression Tree)
library(rpart.plot) # better tool to visualze CART tree
library(party) # conditional inference tree (stopping criterion is based on permutation test; problem of early stoppoing)
library(partykit) # visualize party object
library(randomForest) # random Forest; could be slow
library(ranger) # C++ improvement on randomForest; much faster for tuning paremeter
library(gbm) # gradient boosting machine (boosting)
library(plotmo) 
library(pdp) # create partial dependence plot
library(lime)
```

### CART and CIT (caret)

CART classfication tree
```{r}
set.seed(1234)
rpart.fit.bin <- train(user_rating~., apple2, 
                   subset = trRows2,
                   method = "rpart",
                   tuneGrid = data.frame(cp = exp(seq(-6,-3, len = 30))), 
                   trControl = ctrl2,
                   metric = "ROC")

ggplot(rpart.fit.bin, highlight = TRUE)
rpart.fit.bin$bestTune # cp = 0.0037, treesize = 7
rpart.fit.bin$finalModel$cptable 
rpart.plot(rpart.fit.bin$finalModel) 
```

CIT
```{r}
set.seed(1234)
ctree.fit.bin <- train(user_rating~., apple2, 
                   subset = trRows2,
                   method = "ctree",
                   tuneGrid = data.frame(mincriterion = 1-exp(seq(-4, -1, length = 30))), # 1-alpha
                   metric = "ROC",
                   trControl = ctrl2)
ggplot(ctree.fit.bin, highlight = TRUE)
ctree.fit.bin$bestTune # 1- p.value = 0.894
plot(ctree.fit.bin$finalModel)
```

### bagging 

bagging: caret package
```{r}
bag.grid <- expand.grid(mtry = 8, splitrule = "gini", min.node.size = 1:6) 
set.seed(1234)

bag.fit <- train(user_rating~., apple2, 
                 subset = trRows2, 
                 method = "ranger", 
                 tuneGrid = bag.grid, 
                 trControl = ctrl2)
ggplot(bag.fit, highlight = TRUE)

bag.fit$bestTune
```

bagging: ranger package (for variable of importance)
```{r}
set.seed(1234)
rf.final.per <- ranger(user_rating~., apple2[trRows2,],
                        mtry = 2, splitrule = "gini",
                        min.node.size = 6,
                        importance = "permutation",
                        scale.permutation.importance = TRUE)
barplot(sort(ranger::importance(rf.final.per), decreasing = FALSE),
        las = 2, horiz = TRUE, cex.names = 0.7, main = "Permutation OOB for Binary outcome/response",
        col = colorRampPalette(colors = c("darkred","white","darkblue"))(8))

set.seed(1234)
rf.final.imp <- ranger(user_rating~., apple2[trRows2,],
                        mtry = 2, splitrule = "gini",
                        min.node.size = 6,
                        importance = "impurity")
barplot(sort(ranger::importance(rf.final.imp), decreasing = FALSE),
        las = 2, horiz = TRUE, cex.names = 0.7, main = "Impurity (RSS) for Binary outcome/response",
        col = colorRampPalette(colors = c("darkred","white","darkblue"))(8))
```

### random forest

caret 
```{r}
# Try more if possible
rf.grid <- expand.grid(mtry = 1:8,
                       splitrule = "gini",
                       min.node.size = 1:6)
set.seed(1234)
rf.fit <- train(user_rating~., apple2,
                subset = trRows2,
                method = "ranger",
                tuneGrid = rf.grid,
                metric = "ROC",
                trControl = ctrl2)

ggplot(rf.fit, highlight = TRUE)
rf.fit$bestTune
```

ranger (for variable of importance)
```{r}
set.seed(1234)
rf.final.per <- ranger(user_rating~., apple2[trRows2,],
                        mtry = 2, splitrule = "gini",
                        min.node.size = 6,
                        importance = "permutation",
                        scale.permutation.importance = TRUE)
barplot(sort(ranger::importance(rf.final.per), decreasing = FALSE),
        las = 2, horiz = TRUE, cex.names = 0.7, main = "Permutation OOB for Binary outcome/response",
        col = colorRampPalette(colors = c("darkred","white","darkblue"))(8))

set.seed(1234)
rf.final.imp <- ranger(user_rating~., apple2[trRows2,],
                        mtry = 2, splitrule = "gini",
                        min.node.size = 6,
                        importance = "impurity")
barplot(sort(ranger::importance(rf.final.imp), decreasing = FALSE),
        las = 2, horiz = TRUE, cex.names = 0.7, main = "Impurity (RSS) for Binary outcome/response",
        col = colorRampPalette(colors = c("darkred","white","darkblue"))(8))
```

### boosting & adaboost

boosting (caret)
```{r}
# Try more
gbm.grid <- expand.grid(n.trees = c(2000, 3000, 4000), interaction.depth = 1:10,
                        shrinkage = c(0.001,0.003,0.005), n.minobsinnode = 1)
set.seed(1234)
gbm.fit <- train(user_rating~., apple2, subset = trRows2,
                 method = "gbm",
                 distribution = "bernoulli",
                 tuneGrid = gbm.grid,
                 trControl = ctrl2,
                 metric = "ROC",
                 verbose = FALSE)
ggplot(gbm.fit, highlight = TRUE)
gbm.fit$bestTune

summary(gbm.fit$finalModel, las = 2, cBars = 8, cex.names = 0.8)
```

adaboost (caret)
```{r}
gbm.grid3 <- expand.grid(n.trees = c(2000, 3000, 4000), interaction.depth = 1:10,
                        shrinkage = c(0.001,0.003,0.005), n.minobsinnode = 1)
set.seed(1234)
gbm.fit3 <- train(user_rating~., apple2, subset = trRows2,
                 method = "gbm",
                 distribution = "adaboost",
                 tuneGrid = gbm.grid3,
                 trControl = ctrl2,
                 metric = "ROC",
                 verbose = FALSE)
ggplot(gbm.fit3, highlight = TRUE)
gbm.fit3$bestTune

summary(gbm.fit3$finalModel, las = 2, cBars = 8, cex.names = 0.8)
```

### SVM (caret)

linear: SVM linear
```{r}
library(e1071)
set.seed(1234)
svml.fit <- train(user_rating~., 
                  data = apple2,
                  subset = trRows2,
                  method = "svmLinear2", # use method from e1017 package; linear SVM
                  preProcess = c("center", "scale"), # very important
                  tuneGrid = data.frame(cost = exp(seq(-5,2,len=30))),
                  trControl = ctrl1)

ggplot(svml.fit, highlight = TRUE) # use accuracy and Kappa to evaluate; no probability; no ROC curve
svml.fit$bestTune # 0.66
```

radial kernel: nonlinear SVM
```{r}
# try as much as possible the tuning grid
svmr.grid <- expand.grid(C = exp(seq(-5,2,len=20)),
                         sigma = exp(seq(-8,-3,len=5))) # gamma in SVM function
set.seed(1234)             
svmr.fit <- train(user_rating~., data = apple2, 
                  subset = trRows2,
                  method = "svmRadial", # from package kernal lab
                  preProcess = c("center", "scale"),
                  tuneGrid = svmr.grid,             
                  trControl = ctrl1)
 
ggplot(svmr.fit, highlight = TRUE) 
```

## compare across methods

```{r}
resamp <- resamples(list(CART = rpart.fit.bin, # CART tree
                          CIT = ctree.fit.bin, # CIT tree
                          bag = bag.fit,  # bagging
                          rf = rf.fit,    # random forest
                          gbmB = gbm.fit, # bernulli boosting
                         gbmA = gbm.fit3,
                         glm = model.glm,
                         glmn = model.glmn,
                         lda = model.lda,
                         qda = model.qda,
                         nb = model.nb,
                         knn = model.knn))
summary(resamp)
bwplot(resamp, metric =  "ROC")

resamp.svm <- resamples(list(svmr = svmr.fit, svml = svml.fit))
summary(resamp.svm)
bwplot(resamp.svm)
```


## predict on test data
```{r}
# CART tree
rpart.pred <- predict(rpart.fit.bin, newdata = apple2[-trRows2,], type = "prob")[,2] 
# ctree
rpartc.pred <- predict(ctree.fit.bin, newdata = apple2[-trRows2,], type = "prob")[,2]
# bagging
bagging.pred <- predict(bag.fit, newdata = apple2[-trRows2,], type = "prob")[,2]
# random forest
rf.pred <- predict(rf.fit, newdata = apple2[-trRows2,], type = "prob")[,2] 
# Binomial loss boosting
gbmB.pred <- predict(gbm.fit, newdata = apple2[-trRows2,], type = "prob")[,2]
# Ada boosting
gbmA.pred <- predict(gbm.fit3, newdata = apple2[-trRows2,],type = "prob")[,2]
# glm
glm.pred <-  predict(model.glm, newdata = apple2[-trRows2,],type = "prob")[,2]
# glmn
glmn.pred <-  predict(model.glmn, newdata = apple2[-trRows2,],type = "prob")[,2]
# lda
lda.pred <-  predict(model.lda, newdata = apple2[-trRows2,],type = "prob")[,2]
# qda
qda.pred <-  predict(model.qda, newdata = apple2[-trRows2,],type = "prob")[,2]
# nb
nb.pred <- predict(model.nb, newdata = apple2[-trRows2,],type = "prob")[,2]
# knn
knn.pred <- predict(model.knn, newdata = apple2[-trRows2,],type = "prob")[,2]
```

ROC
```{r}
roc.rpart <- roc(apple2$user_rating[-trRows2], rpart.pred) 
roc.rpartc <- roc(apple2$user_rating[-trRows2], rpartc.pred) 
roc.bag <- roc(apple2$user_rating[-trRows2], bagging.pred)
roc.rf <- roc(apple2$user_rating[-trRows2], rf.pred) 
roc.gbmB <- roc(apple2$user_rating[-trRows2], gbmB.pred)
roc.gbmA <- roc(apple2$user_rating[-trRows2], gbmA.pred) 
roc.glm <- roc(apple2$user_rating[-trRows2], glm.pred)
roc.glmn <- roc(apple2$user_rating[-trRows2], glmn.pred)
roc.lda <- roc(apple2$user_rating[-trRows2], lda.pred)
roc.qda <- roc(apple2$user_rating[-trRows2], qda.pred)
roc.nb <- roc(apple2$user_rating[-trRows2], nb.pred)
roc.knn <- roc(apple2$user_rating[-trRows2], knn.pred)

plot(roc.rpart)
plot(roc.rpartc, add = TRUE, col = 2) 
plot(roc.bag, add = TRUE, col = 3) 
plot(roc.rf, add = TRUE, col = 4) 
plot(roc.gbmB, add = TRUE, col = 5) 
plot(roc.gbmA, add = TRUE, col = 6)
plot(roc.glm, add = TRUE, col = 7)
plot(roc.glmn, add = TRUE, col = 8)
plot(roc.lda, add = TRUE, col = 9)
plot(roc.qda, add = TRUE, col = 10)
plot(roc.nb, add = TRUE, col = 11)
plot(roc.knn, add = TRUE, col = 12)

auc <- c(roc.rpart$auc[1], roc.rpartc$auc[1], roc.bag$auc[1], roc.rf$auc[1], roc.gbmB$auc[1], roc.gbmA$auc[1],
         roc.glm$auc[1], roc.glmn$auc[1], roc.lda$auc[1], roc.qda$auc[1], roc.nb$auc[1], roc.knn$auc[1])
modelNames <- c("rpart","ctree","bag","rf","gbmB","gbmA", "glm", "glmnet", "lda", "qda", "nb", "knn") 
legend("bottomright", legend = paste0(modelNames, ": ", round(auc,3)), col = 1:12, lwd = 2)
```


## visualiza final model: boosting

```{r}
summary(gbm.fit$finalModel, las = 2, cBars = 19, cex.names = 0.6)
```

partial dependence plot (global marginal effect)
```{r}
library(pdp)
library(lime)

# user rating version
gbm.fit %>% 
  partial(pred.var = "user_rating_ver", 
          grid.resolution = 100,
          prob = TRUE) %>%
  autoplot(rug = TRUE, train = apple2[trRows2,]) +
  ggtitle("Boosting") 

```

local effect
```{r}
gbm.fit %>% 
  partial(pred.var = "user_rating_ver", 
          grid.resolution = 100,
          ice = TRUE,
          prob = TRUE) %>%
  autoplot(train = apple2[trRows2,], alpha = .1, 
           center = TRUE) +
  ggtitle("Boosting, centered") 
```


## Cluster and PCA (entire dataset)

```{r}
library(factoextra) # fviz_() functions
library(gridExtra)
library(gplots)
```

prepare data
```{r}
dat <- read_csv("./data/AppleStore.csv") %>%
  janitor::clean_names() %>%
  dplyr::select(-c(x1, id, currency, ver)) %>%
  mutate(size_bytes = round(size_bytes * 1e-6),
         cont_rating = factor(cont_rating, levels = c("4+", "9+","12+","17+")),
         user_rating_bin = factor(ifelse(user_rating >= 4, "high", "med.low"), levels = c("med.low","high"))) %>%
  rename(size_megabytes = size_bytes) %>%
  filter(rating_count_tot != 0,
         user_rating_ver != 0) %>% #Remove apps with no user rating
  mutate(prime_genre = as.integer(ifelse(prime_genre == "Games", 1, 0))) %>% 
  dplyr::select(-rating_count_tot, -rating_count_ver, -vpp_lic) %>% 
  distinct(track_name, .keep_all = TRUE)

str(dat)
dat1 <- dat[, c(2,3, 5,8:10)]
str(dat1)
dat1 <- scale(dat1) # very important to scale data first
rownames(dat1) <- dat$track_name # for dendrogram plotting
```

### K means clustering

```{r}
# this function determines the optimal number of clusters(for reference, there is no truth)
fviz_nbclust(dat1,
             FUNcluster = kmeans,
             method = "silhouette") # three methods can be used; 
set.seed(1234) # the initial group assignment matters
km <- kmeans(dat1, centers = 2, nstart = 20) # try 20 different initial values

km_vis <- fviz_cluster(list(data = dat1, cluster = km$cluster),  
                       ellipse.type = "convex", # boundary shapes
                       geom = "point",
                       #geom = c("point","text"),
                       labelsize = 5, 
                       palette = "Dark2") + labs(title = "K-means") 

km_vis # Plot fisrt 2 PC

# outlier
dat %>% filter(str_starts(track_name, "Proloquo2Go")) %>% View
```

### Hierarchical clustering

```{r}
hc.complete <- hclust(dist(dat1), method = "complete")
hc.average <- hclust(dist(dat1), method = "average")
hc.single <- hclust(dist(dat1), method = "single")
hc.centroid <- hclust(dist(dat1), method = "centroid")
```

dendrogram

```{r}
fviz_dend(hc.complete, k = 2,   # make cut at forming 3 clusters    
          cex = 0.3, 
          palette = "jco", 
          color_labels_by_k = TRUE, # color the label
          rect = TRUE, rect_fill = TRUE, rect_border = "jco", # draw rectangles
          labels_track_height = 2.5) # label size

ind4.complete <- cutree(hc.complete, 4) # divide into 4 clusters; return the cluster assignment to each observation
ind4.complete
# Who are in the fourth cluster?
dat[ind4.complete == 4,]
```

more details
```{r}
col1 <- colorRampPalette(brewer.pal(9, "GnBu"))(100)
col2 <- colorRampPalette(brewer.pal(3, "Spectral"))(2)

heatmap.2(t(dat1), # use transposed dataset
          col = col1, keysize=.8, key.par = list(cex=.5),
          trace = "none", key = TRUE, cexCol = 0.5, 
          #labCol = as.character(dat[,1]),
          ColSideColors = col2[as.numeric(dat$user_rating_bin) + 1],
          margins = c(5, 10))
```

### PCA

```{r}
pca <- prcomp(dat1) # perform PCA
pca$rotation # PC directions 
pca$sdev # square this will be variation explained by each PC; usually decreasing
pca$rotation %*% diag(pca$sdev) # correlation loading; correlation between each variable to PC direction
corrplot(pca$rotation %*% diag(pca$sdev))

var <- get_pca_var(pca) # get diff information about variables
corrplot::corrplot(var$cor)
```

percent explained bye each PC
```{r}
fviz_eig(pca, addlabels = TRUE)
```

PC components, variable contribution
```{r}
# the contribution of each var = rotation^2 to each PC
a <- fviz_contrib(pca, choice = "var", axes = 1) # PC1
b <- fviz_contrib(pca, choice = "var", axes = 2) # PC2
grid.arrange(a, b, nrow = 2)

# check this on PC1
pca$rotation[,1]^2
```


```{r}
# on the plot, the x axis(PC1) is correlation loading of each variable to PC1
fviz_pca_biplot(pca, axes = c(1,2), # number of PC, use first two
                habillage = dat$user_rating_bin, # groups
                label = c("var"), # only label variable names
                addEllipses = TRUE) # boundary of groups
# legendary tend to have high special attack, attack and defence

fviz_pca_var(pca, col.var = "steelblue", repel = TRUE) # if p = 2, then a circle

# for individual
fviz_pca_ind(pca,
             habillage = dat$user_rating_bin,
             label = "none",
             addEllipses = TRUE)
```

## Revise last time MARS model

```{r}
library(earth)

apple3 <- read_csv("./data/AppleStore.csv") %>%
  janitor::clean_names() %>%
  dplyr::select(-c(x1, id, track_name, currency, ver)) %>%
  mutate(size_bytes = round(size_bytes * 1e-6),
         cont_rating = factor(cont_rating, levels = c("4+", "9+","12+","17+"))) %>%
  rename(size_megabytes = size_bytes) %>%
  filter(rating_count_tot != 0,
         user_rating_ver != 0) %>% #Remove apps with no user rating
  mutate(prime_genre = as.integer(ifelse(prime_genre == "Games", 1, 0))) %>% 
  dplyr::select(-rating_count_tot, -rating_count_ver, -vpp_lic)

mars_grid <- expand.grid(degree = 1:2,  # to include interaction or not
                         nprune = 2:20) # how many variables you want to include
mars_grid


set.seed(1234)
mars.fit <- train(user_rating ~., data = apple3,
                  subset = trRows,
                  method = "earth",
                  tuneGrid = mars_grid,
                  trControl = ctrl1)

ggplot(mars.fit, highlight = TRUE) # each line is for one degree;

mars.fit$bestTune # model contains 3 variables with interaction terms

coef(mars.fit$finalModel)
```

create partial dependence plots (PDPs) for each feature individually and also an interaction PDP. This is used to examine the marginal effects of predictors

```{r}
p1 <- partial(mars.fit, pred.var = c("user_rating_ver"), grid.resolution = 10) %>% autoplot()

p2 <- partial(mars.fit, pred.var = c("user_rating_ver", "size_megabytes"), grid.resolution = 10) %>% plotPartial(levelplot = TRUE, zlab = "yhat", drape = TRUE, 
            screen = list(z = 20, x = -60))

p3 <- partial(mars.fit, pred.var = c("user_rating_ver", "prime_genre"), grid.resolution = 10) %>% plotPartial(levelplot = TRUE, zlab = "yhat", drape = TRUE, 
            screen = list(z = 20, x = -60))

grid.arrange(p1, p2, p3, ncol = 3)

```


