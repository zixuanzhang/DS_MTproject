---
title: "EDA"
author: "Eleanor Zhang"
date: "3/23/2019"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width = 7,fig.asp = .7,out.width = "90%",
                      message = FALSE, warning = FALSE)
library(tidyverse)
library(caret)
library(boot)
library(corrplot)
library(pls)
library(glmnet)
library(splines)
library(mgcv)
```

## Read and clean data 

Read data
```{r}
apple <- read_csv("./data/AppleStore.csv") %>% 
  janitor::clean_names()

str(apple) # 7197 observations 17 variables (including response)
unique(apple$currency) # all price on USD unit, so remove this

apple <- apple %>% 
  select(-c(x1, track_name, id, currency, ver)) %>% 
  mutate(size_bytes = round(size_bytes * 1e-6)) # convert to megabytes

summary(apple)
anyNA(apple) # no missing values
str(apple) # select 11 covariables 
```


Response: user_rating  

__user_rating__: Average User Rating value (for all version)

Predictors: 

1.	size_bytes: Size (in Bytes)
2.	price: Price amount ($)
3.	rating_count_tot: User Rating counts (for all version)
4.	rating_count_ver: User Rating counts (for current version)
5.	user_rating_ver: Average User Rating value (for current version)
6.	cont_rating: Content Rating
7.	prime_genre: Primary Genre
8.	sup_devices.num: Number of supporting devices
9.	ipadSc_urls.num: Number of screenshots shown for display
10.	lang.num: Number of supported languages
11.	vpp_lic: Vpp Device Based Licensing Enabled

## EDA

```{r}
# predictor pool
x <- model.matrix(user_rating ~., data = apple)[,-1] # remove the intercept column; design matrix
x
dim(x)
# response
y <- apple$user_rating
unique(y)
```

feature plot
```{r eval=FALSE}
theme1 <- trellis.par.get()
theme1$plot.symbol$col <- rgb(.2, .4, .2, .5)
theme1$plot.symbol$pch <- 16
theme1$plot.line$col <- rgb(.8, .1, .1, 1)
theme1$plot.line$lwd <- 2
theme1$strip.background$col <- rgb(.0, .2, .6, .2)
trellis.par.set(theme1)
featurePlot(x, y, plot = "scatter", labels = c("","Y"),
            type = c("p"), layout = c(5,2)) 
```

Look variables distribution
```{r}
# continuous variables
hist(apple$size_bytes)
hist(apple$price) 
hist(apple$rating_count_tot)
hist(apple$rating_count_ver)
hist(apple$user_rating) # response
hist(apple$sup_devices_num)
hist(apple$ipad_sc_urls_num)
hist(apple$lang_num)

table(apple$cont_rating) # categorical
table(apple$prime_genre) # categorical
table(apple$vpp_lic) # binary 

```


correlation plot excluding two factor variables : content rating and genre
```{r}
apple <- apple %>% select(user_rating, everything())
summary(apple)
# pairs(apple[-c(7,8)])

appleCor <-  cor(apple[-c(7,8)]) # correlation matrix of the modified dataset above
summary(appleCor[upper.tri(appleCor)]) # summary
findCorrelation(appleCor, cutoff = .75)
appleCor

appleCor > 0.75 # user rating (all version) and user rating (current version)

corrplot::corrplot(cor(x), method = "circle",tl.cex = 0.5)

table(apple$prime_genre) # games are oversampled
table(apple$cont_rating) # 4+ is oversampled
```

Comment: correlation between covariates are not significant; user rating and user rating version are highly correlated. (0.77)
Games, 4+ rating are oversampled

check linear dependency of numerical predictors (no problematic predictors)
```{r}
nzv <- nearZeroVar(apple) # nzv: near zero variance
dim(apple[, -nzv])
```

split into train and test data
```{r}
set.seed(1234)
trRows <- createDataPartition(apple$user_rating, # vector of outcomes
                              p = .75, # percentage of training data (random assign)
                              list = FALSE) # in a matrix form 
trRows

# train data
x_train <- model.matrix(user_rating ~., data = apple[trRows,])[,-1] 
x_train
dim(x_train) # 5759
y_train <- apple[trRows,]$user_rating
unique(y_train)

# test data
x_test <- model.matrix(user_rating ~., data = apple[-trRows,])[,-1] 
x_test
dim(x_test) # 1438
y_test <- apple[-trRows,]$user_rating
unique(y_test)
```


## Fit model

### variable selection: ridge and lasso

1. use glmnet package

ridge regression: we have large number of predictors(including categorical variables, so lots of dummy variables)

use ridge regression from `glmnet` package
```{r}
# ridge regression with 100 tuning parameter values (model formula)
ridge.model <- glmnet(x_train, y_train, alpha = 0, lambda = exp(seq(-10, 6, length = 100)))
```

use 10 fold CV to select tuning para.
```{r}
set.seed(1234)
cv.ridge <- cv.glmnet(x_train, y_train, alpha = 0, lambda = exp(seq(-10, 6, length = 100)), 
                      type.measure = "mse")

# plot CV result as a function of tuning para.
plot(cv.ridge)

# optimal lambda:
cv.ridge$lambda.min # 0.013
# 1SE lambda:
cv.ridge$lambda.1se # 0.2027
```

obtain coefficients of ridge model: optimal and sparse
```{r}
# use the optimal:
best.lambda <- cv.ridge$lambda.min
predict(ridge.model, s = best.lambda, type = "coefficients") # on original scale

# use sparse model:
best.1se.lambda <- cv.ridge$lambda.1se
predict(ridge.model, s = best.1se.lambda, type = "coefficients")
```

predict on ridge model
```{r}
pred.ridge <- predict(ridge.model, s = best.lambda, newx = x_test, type = "response")
# MSE
mean((y_test - pred.ridge)^2) # 0.90
```

Lasso regression from `glmnet` package
```{r}
# ridge regression with 100 tuning parameter values (model formula)
lasso.model <- glmnet(x_train, y_train, alpha = 1, lambda = exp(seq(-10, 6, length = 100)))
```

use 10 fold CV to select tuning para.
```{r}
set.seed(1234)
cv.lasso <- cv.glmnet(x_train, y_train, alpha = 1, lambda = exp(seq(-10, 6, length = 100)), 
                      type.measure = "mse")
plot(cv.lasso)

# optimal lambda:
cv.lasso$lambda.min # 0.013
# 1SE lambda:
cv.lasso$lambda.1se # 0.077
```

obtain coefficients of ridge model: optimal and sparse
```{r}
# use the optimal:
best.lambda <- cv.lasso$lambda.min
predict(lasso.model, s = best.lambda, type = "coefficients") # on original scale

# use sparse model:
best.1se.lambda <- cv.lasso$lambda.1se
predict(lasso.model, s = best.1se.lambda, type = "coefficients")
```

predict on ridge model
```{r}
pred.lasso <- predict(lasso.model, s = best.lambda, newx = x_test, type = "response")
# MSE
mean((y_test - pred.lasso)^2) # 0.9022
```


2. use caret package(better for comparison)

use all data points: since this function will do repeated cv for us, we better feed it with all data we have.

```{r}
ctr1 <- trainControl(method = "repeatedcv", number = 10, repeats = 5)
set.seed(1234)
ridge.fit <- train(x_train, y_train, method = "glmnet", 
                   tuneGrid = expand.grid(alpha = 0,
                                          lambda = exp(seq(-10, 6, length = 100))),
                   trControl = ctr1) # ?? warnings

lasso.fit <- train(x_train, y_train, method = "glmnet", 
                   tuneGrid = expand.grid(alpha = 1,
                                          lambda = exp(seq(-10, 6, length = 100))),
                   trControl = ctr1)
plot(ridge.fit)
plot(lasso.fit)
```

look at model
```{r}
ridge.fit$bestTune # 0.106
lasso.fit$bestTune # 0.011 almost no penalty

# coefficients
predict(ridge.fit$finalModel, s=ridge.fit$bestTune$lambda, type="coefficients")
predict(lasso.fit$finalModel, s=lasso.fit$bestTune$lambda, type="coefficients")
```

elastic net
```{r}
set.seed(1234)
enet.fit <- train(x_train, y_train, method = "glmnet", 
                   tuneGrid = expand.grid(alpha = seq(0, 1, length = 5),
                                          lambda = exp(seq(-10, 6, length = 100))),
                   trControl = ctr1)
enet.fit$bestTune 
plot(enet.fit) 
```
comment: shows alpha = 1 (lasso) with lambda = 0.011 gives the lowest RMSE

compare between lasso, ridge, linear fit
```{r}
set.seed(1234)
lm.fit <- train(x_train, y_train, method = "lm", trControl = ctr1)

resamp <- resamples(list(ridge = ridge.fit, lasso = lasso.fit, lm = lm.fit))
summary(resamp)
parallelplot(resamp, metric = "RMSE")
bwplot(resamp, metric = "RMSE")
```

Comment: lasso and ridge does not improve the linear model. Since we assume the strict linear relationships between covariates and response, the regulation from ridge and lasso did not improve the linear model. This implies that the functional form of predictive model may not be merely linear

test MSE
```{r}
mean((y_test - predict(ridge.fit$finalModel, s=ridge.fit$bestTune$lambda, newx = x_test,
          type="response"))^2) # 0.9062
mean((y_test - predict(lasso.fit$finalModel, s=lasso.fit$bestTune$lambda, newx = x_test,
          type="response"))^2)  # 0.9022
mean((y_test - predict(lm.fit, newdata = x_test))^2) # 0.90
```

### PCR and PLS

```{r}
appleCor[appleCor > 0.5]
```
Based on previous result, correlation between variables are not very significant; only user_rating(all version) and user_rating(current version) is highly correlated

PCR:
```{r}
set.seed(1234)
pcr.fit <- train(x_train, y_train,
                  method = "pcr",
                  tuneLength = 34,
                  trControl = ctr1,
                  scale = TRUE) 

pred.pcr <- predict(pcr.fit$finalModel, newdata = x_test,
                       ncomp = pcr.fit$bestTune[[1]]) # tuning paramter to choose
mean((pred.pcr - y_test)^2)

ggplot(pcr.fit, highlight = TRUE) + theme_bw()
```

PLS:
```{r}
set.seed(1234)
pls.fit <- train(x_train, y_train,
                  method = "pls",
                  tuneLength = 34,
                  trControl = ctr1,
                  scale = TRUE) 

pred.pls <- predict(pls.fit$finalModel, newdata = x_test,
                       ncomp = pls.fit$bestTune[[1]]) # tuning paramter to choose
mean((pred.pls - y_test)^2) # 0.9

ggplot(pls.fit, highlight = TRUE) + theme_bw()
```

compare btw all models above
```{r}
resamp <- resamples(list(lasso = lasso.fit, 
                         ridge = ridge.fit, 
                         pcr = pcr.fit, 
                         pls = pls.fit,
                         lm = lm.fit)) 
summary(resamp)

bwplot(resamp, metric = "RMSE")
```

Comment: models performance are very alike linear regression. 

### Beyond linear

add nonlinear components in the model

#### Polynomials

CV to compare models up to d = 4 and make plot

add higher order on size_bytes
```{r}
ctrl2 <- trainControl(method = "cv", number = 10)

set.seed(1234)
lmFit1 <- train(user_rating ~ size_bytes,
                data = apple[trRows, ], 
                method = "lm",
                trControl = ctrl2) 
lmFit2 <- train(user_rating ~ poly(size_bytes,2),
                data = apple[trRows, ], 
                method = "lm",
                trControl = ctrl2)
lmFit3 <- train(user_rating ~ poly(size_bytes,3),
                data = apple[trRows, ], 
                method = "lm",
                trControl = ctrl2)
lmFit4 <- train(user_rating ~ poly(size_bytes,4),
                data = apple[trRows, ], 
                method = "lm",
                trControl = ctrl2)

resamp <- resamples(list(d1 = lmFit1, d2 = lmFit2, d3 = lmFit3, d4 = lmFit4)) 
summary(resamp) # MSE

bwplot(resamp, metric = "RMSE")
```

comment: No improve

add higher order on price
```{r}
set.seed(1234)
lmFit1 <- train(user_rating ~ price,
                data = apple[trRows, ], 
                method = "lm",
                trControl = ctrl2) 
lmFit2 <- train(user_rating ~ poly(price,2),
                data = apple[trRows, ], 
                method = "lm",
                trControl = ctrl2)
lmFit3 <- train(user_rating ~ poly(price,3),
                data = apple[trRows, ], 
                method = "lm",
                trControl = ctrl2)
lmFit4 <- train(user_rating ~ poly(price,4),
                data = apple[trRows, ], 
                method = "lm",
                trControl = ctrl2)

resamp <- resamples(list(d1 = lmFit1, d2 = lmFit2, d3 = lmFit3, d4 = lmFit4)) 
summary(resamp) # MSE

bwplot(resamp, metric = "RMSE")
```
comment: did not improve much, so keep d = 1

add higher order on rating_count_tot
```{r}
set.seed(1234)
lmFit1 <- train(user_rating ~ rating_count_tot,
                data = apple[trRows, ], 
                method = "lm",
                trControl = ctrl2) 
lmFit2 <- train(user_rating ~ poly(rating_count_tot,2),
                data = apple[trRows, ], 
                method = "lm",
                trControl = ctrl2)
lmFit3 <- train(user_rating ~ poly(rating_count_tot,3),
                data = apple[trRows, ], 
                method = "lm",
                trControl = ctrl2)
lmFit4 <- train(user_rating ~ poly(rating_count_tot,4),
                data = apple[trRows, ], 
                method = "lm",
                trControl = ctrl2)

resamp <- resamples(list(d1 = lmFit1, d2 = lmFit2, d3 = lmFit3, d4 = lmFit4)) 
summary(resamp) # MSE

bwplot(resamp, metric = "RMSE")
```

Comment: does not improve

add higher order on lang_num
```{r}
set.seed(1234)
lmFit1 <- train(user_rating ~ lang_num,
                data = apple[trRows, ], 
                method = "lm",
                trControl = ctrl2) 
lmFit2 <- train(user_rating ~ poly(lang_num,2),
                data = apple[trRows, ], 
                method = "lm",
                trControl = ctrl2)
lmFit3 <- train(user_rating ~ poly(lang_num,3),
                data = apple[trRows, ], 
                method = "lm",
                trControl = ctrl2)
lmFit4 <- train(user_rating ~ poly(lang_num,4),
                data = apple[trRows, ], 
                method = "lm",
                trControl = ctrl2)

resamp <- resamples(list(d1 = lmFit1, d2 = lmFit2, d3 = lmFit3, d4 = lmFit4)) 
summary(resamp) # MSE

bwplot(resamp, metric = "RMSE")
```

Conclusion: add polynomial component of lang_num and size_bytes does not make too much difference

Comment: it does not really improve too much; so better keep d = 1

check anova
```{r}
fit1 <- lm(user_rating~size_bytes, data = apple[trRows,])  # y ~ X 
fit2 <- lm(user_rating~poly(size_bytes,2), data = apple[trRows,]) # y ~ X + X^2
fit3 <- lm(user_rating~poly(size_bytes,3), data = apple[trRows,]) # y ~ X + X^2 + X^3
fit4 <- lm(user_rating~poly(size_bytes,4), data = apple[trRows,]) # y ~ X + X^2 + X^3 + X^4
anova(fit1, fit2, fit3, fit4)

fit1 <- lm(user_rating~lang_num, data = apple[trRows,])  # y ~ X 
fit2 <- lm(user_rating~poly(lang_num,2), data = apple[trRows,]) # y ~ X + X^2
fit3 <- lm(user_rating~poly(lang_num,3), data = apple[trRows,]) # y ~ X + X^2 + X^3
fit4 <- lm(user_rating~poly(lang_num,4), data = apple[trRows,])
anova(fit1, fit2, fit3, fit4)
```

Comment: ANOVA result suggest adding d = 3 to lang_num and d = 4 on size_bytes
How to decide on this?

#### Smoothing splines

fit smoothing spline on size_bytes
```{r}
p <- ggplot(data = apple[trRows,], aes(x = size_bytes, y = user_rating)) +
     geom_point(color = rgb(.2, .4, .2, .5))
p

fit.ss <- smooth.spline(apple[trRows,]$size_bytes, # predictor (univariate)
                        apple[trRows,]$user_rating)  # response
fit.ss$df 

# look at the range
sizelims <- range(apple$size_bytes)

# create a sequence of observations pgg45
size.grid <- seq(from = sizelims[1],to = sizelims[2], 100)

pred.ss <- predict(fit.ss, x = size.grid) # specify x; 
# but we did not calculate CI in this function
pred.ss.df <- data.frame(pred = pred.ss$y,
                         size = size.grid)

p +
geom_line(aes(x = size, y = pred), data = pred.ss.df,
          color = rgb(.8, .1, .1, 1)) + theme_bw()

```

fit smoothing spline to lang_num
```{r}
fit.ss <- smooth.spline(apple[trRows,]$lang_num, # predictor (univariate)
                        apple[trRows,]$user_rating)  # response
fit.ss$df 

# look at the range
langlims <- range(apple$lang_num)

# create a sequence of observations pgg45
lang.grid <- seq(from = langlims[1],to = langlims[2])
lang.grid
pred.ss <- predict(fit.ss, x = lang.grid) # specify x; 
# but we did not calculate CI in this function
pred.ss.df <- data.frame(pred = pred.ss$y,
                         lang = lang.grid)
pred.ss.df

ggplot(data = apple[trRows,], aes(x = lang_num, y = user_rating)) +
     geom_point(color = rgb(.2, .4, .2, .5))+
geom_line(aes(x = lang, y = pred), data = pred.ss.df,
          color = rgb(.8, .1, .1, 1)) + theme_bw()
```

#### local regression

```{r}
fit.loess <- loess(user_rating ~ size_bytes, data = apple[trRows,])
summary(fit.loess)
pred.loess <- predict(fit.loess, newdata = data.frame(size_bytes = size.grid))

pred.loess.df <- data.frame(pred = pred.loess,
                            size = size.grid)

p + geom_line(aes(x = size, y = pred), data = pred.loess.df,
              color = rgb(.8, .1, .1, 1)) + theme_bw()
```

#### GAM

mgcv package
```{r}
# Start with linear model; do not assume nonlinear trait
gam.m1 <- gam(user_rating ~ size_bytes + price + rating_count_tot + rating_count_ver +
                user_rating_ver + cont_rating + prime_genre + sup_devices_num +
                ipad_sc_urls_num + lang_num + vpp_lic , data = apple[trRows,]) 
summary(gam.m1)

# add one non-linear component to size bytes
gam.m2 <- gam(user_rating ~ s(size_bytes) + price + rating_count_tot + rating_count_ver +
                user_rating_ver + cont_rating + prime_genre + sup_devices_num +
                ipad_sc_urls_num + lang_num + vpp_lic , data = apple[trRows,])
summary(gam.m2) 

#  add one non-linear component to lang_num
gam.m3 <- gam(user_rating ~ s(size_bytes) + price + rating_count_tot + rating_count_ver +
                user_rating_ver + cont_rating + prime_genre + sup_devices_num +
                ipad_sc_urls_num + s(lang_num) + vpp_lic , data = apple[trRows,])

anova(gam.m1, gam.m2, gam.m3, test = "F")
```

use logistic regression to fit the model
can add penalty term on the regression model
gam for logistic regression

DALEX package

can use naive bayes for both quantitative and qualitative variables
